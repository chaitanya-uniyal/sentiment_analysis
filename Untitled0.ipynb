{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2_QtGCeB0bNI",
        "outputId": "8db880a0-becf-4d73-9db5-39cca8803a61"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install contractions\n",
        "!pip install textsearch\n",
        "!pip install tqdm\n",
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XBsozyyngmE-",
        "outputId": "7424affb-a191-48cb-9d6e-141fbd1ddd74"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: contractions in /usr/local/lib/python3.10/dist-packages (0.1.73)\n",
            "Requirement already satisfied: textsearch>=0.0.21 in /usr/local/lib/python3.10/dist-packages (from contractions) (0.0.24)\n",
            "Requirement already satisfied: anyascii in /usr/local/lib/python3.10/dist-packages (from textsearch>=0.0.21->contractions) (0.3.2)\n",
            "Requirement already satisfied: pyahocorasick in /usr/local/lib/python3.10/dist-packages (from textsearch>=0.0.21->contractions) (2.1.0)\n",
            "Requirement already satisfied: textsearch in /usr/local/lib/python3.10/dist-packages (0.0.24)\n",
            "Requirement already satisfied: anyascii in /usr/local/lib/python3.10/dist-packages (from textsearch) (0.3.2)\n",
            "Requirement already satisfied: pyahocorasick in /usr/local/lib/python3.10/dist-packages (from textsearch) (2.1.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.66.4)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "import contractions\n",
        "from bs4 import BeautifulSoup\n",
        "import numpy as np\n",
        "import re\n",
        "import tqdm\n",
        "import unicodedata\n",
        "\n",
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.layers import Dropout\n",
        "from tensorflow.keras.layers import Flatten\n",
        "from tensorflow.keras.layers import Conv1D\n",
        "from tensorflow.keras.layers import MaxPooling1D\n",
        "from tensorflow.keras.layers import Embedding\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing import sequence\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "from tensorflow.keras.regularizers import l2\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import f1_score, accuracy_score,confusion_matrix,classification_report\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "# Time\n",
        "import time\n",
        "import datetime\n",
        "\n",
        "%matplotlib inline\n",
        "\n",
        "# fix random seed for reproducibility\n",
        "seed = 3541\n",
        "np.random.seed(seed)\n",
        "\n",
        "# Function definitions for preprocessing\n",
        "def strip_html_tags(text):\n",
        "    soup = BeautifulSoup(text, \"html.parser\")\n",
        "    [s.extract() for s in soup(['iframe', 'script'])]\n",
        "    stripped_text = soup.get_text()\n",
        "    stripped_text = re.sub(r'[\\r|\\n|\\r\\n]+', '\\n', stripped_text)\n",
        "    return stripped_text\n",
        "\n",
        "def remove_accented_chars(text):\n",
        "    text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
        "    return text\n",
        "\n",
        "def pre_process_corpus(docs):\n",
        "    norm_docs = []\n",
        "    for doc in docs:\n",
        "        doc = strip_html_tags(doc)\n",
        "        doc = doc.translate(doc.maketrans(\"\\n\\t\\r\", \"   \"))\n",
        "        doc = doc.lower()\n",
        "        doc = remove_accented_chars(doc)\n",
        "        doc = contractions.fix(doc)\n",
        "        doc = re.sub(r'[^a-zA-Z0-9\\s]', '', doc, re.I|re.A)\n",
        "        doc = re.sub(' +', ' ', doc)\n",
        "        doc = doc.strip()\n",
        "        norm_docs.append(doc)\n",
        "    return norm_docs\n",
        "\n",
        "# Load the training data\n",
        "\n",
        "dataset_train = pd.read_csv('/content/drive/MyDrive/yelp/train.csv')\n",
        "dataset_test = pd.read_csv('/content/drive/MyDrive/yelp/test.csv')\n",
        "dataset_val =  pd.read_csv('/content/drive/MyDrive/yelp/test.csv')\n",
        "\n",
        "# build+shuffle train and test and validation datasets\n",
        "\n",
        "val = dataset_val.sample(frac=1)\n",
        "train = dataset_train.sample(frac=1)\n",
        "test = dataset_test.sample(frac=1)\n",
        "\n",
        "test = dataset_test.iloc[:50000,:]\n",
        "train = dataset_train.iloc[50000:,:]\n",
        "val = dataset_train.iloc[:50000,:]\n",
        "train = dataset_train.iloc[50000:,:]\n",
        "train = dataset_train.iloc[:100000,:]\n",
        "\n",
        "X_train = train['review_text'].values\n",
        "y_train = train['class_index'].values\n",
        "\n",
        "X_val = val['review_text'].values\n",
        "y_val = val['class_index'].values\n",
        "\n",
        "X_test = test['review_text'].values\n",
        "y_test = test['class_index'].values\n",
        "\n",
        "\n",
        "\n",
        "X_train = pre_process_corpus(X_train)\n",
        "X_val = pre_process_corpus(X_val)\n",
        "X_test = pre_process_corpus(X_test)\n",
        "\n",
        "t = Tokenizer(oov_token='<UNK>')\n",
        "# fit the tokenizer on train documents\n",
        "t.fit_on_texts(X_train)\n",
        "t.word_index['<PAD>'] = 0\n",
        "\n",
        "X_train = t.texts_to_sequences(X_train)\n",
        "X_test = t.texts_to_sequences(X_test)\n",
        "X_val = t.texts_to_sequences(X_val)\n",
        "\n",
        "\n",
        "MAX_SEQUENCE_LENGTH = 220\n",
        "# pad dataset to a maximum review length in words\n",
        "X_train = sequence.pad_sequences(X_train, maxlen=MAX_SEQUENCE_LENGTH)\n",
        "X_test = sequence.pad_sequences(X_test, maxlen=MAX_SEQUENCE_LENGTH)\n",
        "X_val = sequence.pad_sequences(X_val, maxlen=MAX_SEQUENCE_LENGTH)\n",
        "\n",
        "le = LabelEncoder()\n",
        "num_classes=2 # positive -> 1, negative -> 0\n",
        "\n",
        "y_train = le.fit_transform(y_train)\n",
        "y_test = le.transform(y_test)\n",
        "y_val = le.transform(y_val)\n",
        "\n",
        "VOCAB_SIZE = len(t.word_index)\n",
        "\n",
        "EMBED_SIZE = 300\n",
        "#optimizer = tf.keras.optimizers.Adam(learning_rate=0.01)\n",
        "\n",
        "\n",
        "\n",
        "# Load the saved model\n",
        "model = tf.keras.models.load_model('/content/drive/MyDrive/yelp/Binary_Classification_with_CNN___2.keras')\n",
        "\n",
        "# Define the input string\n",
        "input_string = \"this is awsome but it need improvement\"\n",
        "\n",
        "# Preprocess the input string\n",
        "preprocessed_input = pre_process_corpus([input_string])\n",
        "\n",
        "# Convert the input string to sequence\n",
        "input_sequence = t.texts_to_sequences(preprocessed_input)\n",
        "\n",
        "# Pad the sequence\n",
        "padded_input_sequence = sequence.pad_sequences(input_sequence, maxlen=MAX_SEQUENCE_LENGTH)\n",
        "\n",
        "# Make a prediction\n",
        "prediction = model.predict(padded_input_sequence)\n",
        "\n",
        "# Print the output\n",
        "print(\"Prediction:\", prediction)\n",
        "print(\"Sentiment:\", \"Positive\" if prediction[0] > 0.5 else \"Negative\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C1oIWQfD-kUN",
        "outputId": "1cf38631-449d-4a6d-cdb6-8c6dae91f822"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-8-9e5d87de2047>:41: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
            "  soup = BeautifulSoup(text, \"html.parser\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 448ms/step\n",
            "Prediction: [[0.9467487]]\n",
            "Sentiment: Positive\n"
          ]
        }
      ]
    }
  ]
}